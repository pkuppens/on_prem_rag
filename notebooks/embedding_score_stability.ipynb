{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Embedding Score Stability: Adding Documents Should Not Change Absolute Scores\n",
    "\n",
    "This notebook tests a fundamental property of vector similarity search:\n",
    "\n",
    "> **Hypothesis**: Adding new documents to a ChromaDB collection should NOT change the absolute\n",
    "> similarity scores of already-indexed documents. Only the relative ranking (position) may change\n",
    "> if the new documents contain better-matching content.\n",
    "\n",
    "This matters because it validates that our RAG system's scoring is **stable and trustworthy** -\n",
    "a score of 0.85 today should still mean 0.85 after ingesting more documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import setup_notebook  # noqa: F401 - fixes import path\n",
    "from rag_pipeline.config.parameter_sets import get_param_set\n",
    "from rag_pipeline.core.embeddings import chunk_pdf, embed_chunks, query_embeddings\n",
    "from rag_pipeline.utils.directory_utils import get_test_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We use two topically different PDFs and a fresh temporary ChromaDB directory to ensure isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_test_data_dir()\n",
    "params = get_param_set(\"fast\")\n",
    "\n",
    "# Document A: first PDF to index (baseline)\n",
    "# Document B: added later to test score stability\n",
    "doc_a_path = test_data / \"2303.18223v16.pdf\"\n",
    "doc_b_path = test_data / \"2005.11401v4.pdf\"\n",
    "\n",
    "# Limit pages for faster processing\n",
    "MAX_PAGES = 5\n",
    "\n",
    "# Queries to test (mix of topics relevant to either document)\n",
    "TEST_QUERIES = [\n",
    "    \"Large Language Models\",\n",
    "    \"Healthcare applications\",\n",
    "    \"Machine learning training\",\n",
    "    \"Neural network architecture\",\n",
    "]\n",
    "\n",
    "# Use a large top_k to capture as many Document A chunks as possible in both rounds\n",
    "TOP_K = 100\n",
    "\n",
    "# Floating-point tolerance for score comparison\n",
    "SCORE_TOLERANCE = 1e-6\n",
    "\n",
    "# Create a fresh temporary directory for this experiment\n",
    "tmp_dir = Path(tempfile.mkdtemp(prefix=\"score_stability_\"))\n",
    "persist_dir = tmp_dir / \"chroma\"\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Document A: {doc_a_path.name}\")\n",
    "print(f\"Document B: {doc_b_path.name}\")\n",
    "print(f\"Parameters: {params.chunking.chunk_size} chunk size, {params.chunking.chunk_overlap} overlap\")\n",
    "print(f\"Embedding model: {params.embedding.model_name}\")\n",
    "print(f\"Temp directory: {tmp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 1: Chunk and embed Document A (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_a = chunk_pdf(\n",
    "    doc_a_path,\n",
    "    chunk_size=params.chunking.chunk_size,\n",
    "    chunk_overlap=params.chunking.chunk_overlap,\n",
    "    max_pages=MAX_PAGES,\n",
    ")\n",
    "\n",
    "chunks_a, records_a = embed_chunks(\n",
    "    chunking_a,\n",
    "    params.embedding.model_name,\n",
    "    persist_dir=str(persist_dir),\n",
    "    deduplicate=True,\n",
    ")\n",
    "\n",
    "print(f\"Document A: {chunks_a} chunks created, {records_a} records stored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 2: Query baseline (only Document A in the collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record scores BEFORE adding Document B\n",
    "scores_before: dict[str, dict[str, float]] = {}  # {query: {record_id: score}}\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    results = query_embeddings(\n",
    "        query,\n",
    "        params.embedding.model_name,\n",
    "        persist_dir=str(persist_dir),\n",
    "        top_k=TOP_K,\n",
    "    )\n",
    "\n",
    "    scores_before[query] = {r[\"record_id\"]: r[\"similarity_score\"] for r in results[\"all_results\"]}\n",
    "\n",
    "    n_results = len(results[\"all_results\"])\n",
    "    top_score = results[\"all_results\"][0][\"similarity_score\"] if n_results > 0 else 0\n",
    "    print(f\"Query: {query!r:40s} -> {n_results} results, top score: {top_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline recorded for {len(TEST_QUERIES)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 3: Add Document B to the same collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_b = chunk_pdf(\n",
    "    doc_b_path,\n",
    "    chunk_size=params.chunking.chunk_size,\n",
    "    chunk_overlap=params.chunking.chunk_overlap,\n",
    "    max_pages=MAX_PAGES,\n",
    ")\n",
    "\n",
    "chunks_b, records_total = embed_chunks(\n",
    "    chunking_b,\n",
    "    params.embedding.model_name,\n",
    "    persist_dir=str(persist_dir),\n",
    "    deduplicate=True,\n",
    ")\n",
    "\n",
    "print(f\"Document B: {chunks_b} chunks created\")\n",
    "print(f\"Total records in collection: {records_total}\")\n",
    "print(f\"(was {records_a} before adding Document B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Re-run the same queries (Document A + B in the collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record scores AFTER adding Document B\n",
    "scores_after: dict[str, dict[str, float]] = {}\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    results = query_embeddings(\n",
    "        query,\n",
    "        params.embedding.model_name,\n",
    "        persist_dir=str(persist_dir),\n",
    "        top_k=TOP_K,\n",
    "    )\n",
    "\n",
    "    scores_after[query] = {r[\"record_id\"]: r[\"similarity_score\"] for r in results[\"all_results\"]}\n",
    "\n",
    "    n_results = len(results[\"all_results\"])\n",
    "    top_score = results[\"all_results\"][0][\"similarity_score\"] if n_results > 0 else 0\n",
    "    print(f\"Query: {query!r:40s} -> {n_results} results, top score: {top_score:.4f}\")\n",
    "\n",
    "print(f\"\\nPost-addition scores recorded for {len(TEST_QUERIES)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 5: Compare absolute scores for Document A chunks\n",
    "\n",
    "For every Document A chunk that appears in both the before and after result sets,\n",
    "the absolute similarity score must be identical (within floating-point tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passed = True\n",
    "comparison_data = []  # For visualization later\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    before = scores_before[query]\n",
    "    after = scores_after[query]\n",
    "\n",
    "    # Find Document A chunks present in both result sets\n",
    "    common_ids = set(before.keys()) & set(after.keys())\n",
    "\n",
    "    mismatches = []\n",
    "    for record_id in sorted(common_ids):\n",
    "        score_before = before[record_id]\n",
    "        score_after = after[record_id]\n",
    "        diff = abs(score_before - score_after)\n",
    "\n",
    "        comparison_data.append(\n",
    "            {\n",
    "                \"query\": query,\n",
    "                \"record_id\": record_id,\n",
    "                \"score_before\": score_before,\n",
    "                \"score_after\": score_after,\n",
    "                \"diff\": diff,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if diff > SCORE_TOLERANCE:\n",
    "            mismatches.append((record_id, score_before, score_after, diff))\n",
    "\n",
    "    status = \"PASS\" if not mismatches else \"FAIL\"\n",
    "    print(f\"Query: {query!r:40s} -> {len(common_ids)} common chunks, {status}\")\n",
    "\n",
    "    if mismatches:\n",
    "        all_passed = False\n",
    "        for record_id, sb, sa, d in mismatches:\n",
    "            print(f\"  MISMATCH {record_id}: {sb:.6f} -> {sa:.6f} (diff={d:.2e})\")\n",
    "\n",
    "print()\n",
    "if all_passed:\n",
    "    print(\"ALL QUERIES PASSED: Absolute scores are stable after adding documents.\")\n",
    "else:\n",
    "    print(\"SOME QUERIES FAILED: Scores changed after adding documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard assertion - this cell will fail if scores changed\n",
    "assert all_passed, (\n",
    "    \"Absolute similarity scores changed after adding documents! \"\n",
    "    \"This violates the expected behavior of cosine similarity in ChromaDB.\"\n",
    ")\n",
    "print(f\"Verified: {len(comparison_data)} score comparisons all within tolerance ({SCORE_TOLERANCE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Show how rankings can change\n",
    "\n",
    "While absolute scores stay the same, the ranking position of Document A chunks\n",
    "may change because Document B chunks may now appear with higher scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in TEST_QUERIES:\n",
    "    before = scores_before[query]\n",
    "    after = scores_after[query]\n",
    "\n",
    "    # Rankings: sorted by score descending\n",
    "    rank_before = {rid: i + 1 for i, rid in enumerate(sorted(before, key=before.get, reverse=True))}\n",
    "    rank_after = {rid: i + 1 for i, rid in enumerate(sorted(after, key=after.get, reverse=True))}\n",
    "\n",
    "    # Check Document A chunks that moved in ranking\n",
    "    doc_a_ids = set(before.keys())  # All IDs from before are Document A\n",
    "    moved = []\n",
    "    for rid in sorted(doc_a_ids & set(rank_after.keys())):\n",
    "        rb = rank_before[rid]\n",
    "        ra = rank_after[rid]\n",
    "        if rb != ra:\n",
    "            moved.append((rid, rb, ra))\n",
    "\n",
    "    # Count new Document B chunks that appeared in top results\n",
    "    new_ids = set(after.keys()) - doc_a_ids\n",
    "\n",
    "    print(f\"\\nQuery: {query!r}\")\n",
    "    print(f\"  Document B chunks in results: {len(new_ids)}\")\n",
    "    print(f\"  Document A chunks that changed rank: {len(moved)}\")\n",
    "    if moved:\n",
    "        for rid, rb, ra in moved[:5]:  # Show top 5 rank changes\n",
    "            direction = \"down\" if ra > rb else \"up\"\n",
    "            print(f\"    {rid}: rank {rb} -> {ra} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 7: Visualize score stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Before vs After scores (should be on the diagonal)\n",
    "ax1 = axes[0]\n",
    "before_scores = [d[\"score_before\"] for d in comparison_data]\n",
    "after_scores = [d[\"score_after\"] for d in comparison_data]\n",
    "\n",
    "ax1.scatter(before_scores, after_scores, alpha=0.6, edgecolors=\"black\", linewidth=0.5)\n",
    "\n",
    "# Perfect stability line\n",
    "score_min = min(min(before_scores), min(after_scores))\n",
    "score_max = max(max(before_scores), max(after_scores))\n",
    "ax1.plot([score_min, score_max], [score_min, score_max], \"r--\", label=\"Perfect stability\")\n",
    "\n",
    "ax1.set_xlabel(\"Score before adding Document B\")\n",
    "ax1.set_ylabel(\"Score after adding Document B\")\n",
    "ax1.set_title(\"Absolute Score Stability\")\n",
    "ax1.legend()\n",
    "ax1.set_aspect(\"equal\")\n",
    "\n",
    "# Plot 2: Distribution of score differences\n",
    "ax2 = axes[1]\n",
    "diffs = [d[\"diff\"] for d in comparison_data]\n",
    "ax2.hist(diffs, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "ax2.axvline(x=SCORE_TOLERANCE, color=\"r\", linestyle=\"--\", label=f\"Tolerance ({SCORE_TOLERANCE:.0e})\")\n",
    "ax2.set_xlabel(\"Absolute score difference\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Score Difference Distribution\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embedding_score_stability.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Max difference: {max(diffs):.2e}\")\n",
    "print(f\"Mean difference: {sum(diffs) / len(diffs):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EMBEDDING SCORE STABILITY EXPERIMENT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Document A: {doc_a_path.name} ({records_a} records)\")\n",
    "print(f\"Document B: {doc_b_path.name} ({records_total - records_a} records added)\")\n",
    "print(f\"Total collection size after: {records_total} records\")\n",
    "print(f\"Queries tested: {len(TEST_QUERIES)}\")\n",
    "print(f\"Score comparisons: {len(comparison_data)}\")\n",
    "print(f\"Max score difference: {max(d['diff'] for d in comparison_data):.2e}\")\n",
    "print(f\"Result: {'PASS - Scores are stable' if all_passed else 'FAIL - Scores changed!'}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Conclusion:\")\n",
    "if all_passed:\n",
    "    print(\"  Adding documents to a ChromaDB collection does NOT affect the\")\n",
    "    print(\"  absolute similarity scores of previously indexed documents.\")\n",
    "    print(\"  Only rankings may change when new, better-matching content is added.\")\n",
    "else:\n",
    "    print(\"  UNEXPECTED: Scores changed! This needs investigation.\")\n",
    "print()\n",
    "print(\"This is expected because cosine similarity is computed independently\")\n",
    "print(\"for each query-document pair. The score depends only on the query\")\n",
    "print(\"embedding and the document chunk embedding, not on other documents\")\n",
    "print(\"in the collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "print(f\"Cleaned up {tmp_dir}\")\n",
    "print(\"\\u2705 Embedding score stability experiment completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
