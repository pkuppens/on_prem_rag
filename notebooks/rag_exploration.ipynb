{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring a local RAG system with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION CONSTANTS WITH EXPLANATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Chunk size: Number of characters per text chunk\n",
    "# - Smaller chunks (256-512): Better for precise answers, more granular retrieval\n",
    "# - Larger chunks (1024-2048): Better context, may include irrelevant info\n",
    "# - Rule of thumb: Match your typical question complexity\n",
    "DEFAULT_CHUNK_SIZE = 512\n",
    "\n",
    "# Chunk overlap: Characters shared between adjacent chunks\n",
    "# - Prevents information loss at chunk boundaries\n",
    "# - Usually 10-20% of chunk_size\n",
    "# - Higher overlap = more redundancy but better context preservation\n",
    "DEFAULT_CHUNK_OVERLAP = 50\n",
    "\n",
    "# Embedding model: Converts text to vector representations\n",
    "# - \"BAAI/bge-small-en-v1.5\": Fast, good general performance, 384 dimensions\n",
    "# - \"sentence-transformers/all-MiniLM-L6-v2\": Lighter, 384 dimensions\n",
    "# - \"BAAI/bge-large-en-v1.5\": Better quality, slower, 1024 dimensions\n",
    "# - Choose based on speed vs accuracy tradeoff\n",
    "DEFAULT_EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Retrieval settings\n",
    "# - Top-k: Number of most similar chunks to retrieve per query\n",
    "# - Higher k = more context but potentially more noise\n",
    "DEFAULT_TOP_K = 3\n",
    "\n",
    "# LLM settings\n",
    "DEFAULT_LLM_MODEL = \"mistral\"  # Ollama model name\n",
    "DEFAULT_TEMPERATURE = 0.1      # Lower = more focused, higher = more creative\n",
    "DEFAULT_MAX_TOKENS = 512       # Maximum response length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    ServiceContext, \n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Document\n",
    ")\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.readers.file import PDFReader, DocxReader\n",
    "\n",
    "# Alternative: Use Hugging Face Transformers for local Mistral\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalRAGSystem:\n",
    "    \"\"\"\n",
    "    A complete RAG system that runs locally using LlamaIndex and Mistral.\n",
    "    \n",
    "    This class provides a full implementation of a Retrieval-Augmented Generation\n",
    "    system that can process documents, create vector indices, and answer questions\n",
    "    using local LLMs and embeddings.\n",
    "    \n",
    "    Key features:\n",
    "    - Document loading from multiple formats (PDF, DOCX, TXT, MD)\n",
    "    - Local embedding generation using HuggingFace models\n",
    "    - Local LLM inference using Ollama or HuggingFace\n",
    "    - Source attribution and reference tracking\n",
    "    - Persistent index storage and loading\n",
    "    \n",
    "    Example:\n",
    "        >>> rag = LocalRAGSystem()\n",
    "        >>> documents = rag.load_mixed_documents(\"./data\")\n",
    "        >>> index = rag.create_index(documents, persist_dir=\"./storage\")\n",
    "        >>> result = rag.query(index, \"What is the warranty period?\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = DEFAULT_LLM_MODEL,\n",
    "                 embed_model_name: str = DEFAULT_EMBED_MODEL,\n",
    "                 chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "                 chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
    "                 top_k: int = DEFAULT_TOP_K):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with specified configuration.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the LLM model to use (default: \"mistral\")\n",
    "            embed_model_name: Name of the embedding model\n",
    "            chunk_size: Size of text chunks for processing (default: 512)\n",
    "            chunk_overlap: Overlap between chunks (default: 50)\n",
    "            top_k: Number of chunks to retrieve per query (default: 3)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embed_model_name = embed_model_name\n",
    "        self.model_name = model_name\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Initialize components\n",
    "        self.llm = self._setup_llm()\n",
    "        self.embed_model = self._setup_embeddings()\n",
    "        self.service_context = ServiceContext.from_defaults(\n",
    "            llm=self.llm,\n",
    "            embed_model=self.embed_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Document readers for different file types\n",
    "        self.pdf_reader = PDFReader()\n",
    "        self.docx_reader = DocxReader()\n",
    "        \n",
    "    def _setup_llm(self):\n",
    "        \"\"\"\n",
    "        Setup local LLM - using Ollama (recommended) or HuggingFace.\n",
    "        \n",
    "        Returns:\n",
    "            An LLM instance configured for local inference.\n",
    "            \n",
    "        Note:\n",
    "            First tries to use Ollama, falls back to HuggingFace if Ollama fails.\n",
    "            For Ollama, ensure you have run: ollama pull mistral\n",
    "        \"\"\"\n",
    "        # Option 1: Using Ollama (easier, recommended)\n",
    "        try:\n",
    "            llm = Ollama(\n",
    "                model=self.model_name,\n",
    "                request_timeout=120.0,\n",
    "                temperature=DEFAULT_TEMPERATURE,\n",
    "                system_prompt=(\n",
    "                    \"You are a helpful assistant that answers questions based on \"\n",
    "                    \"the provided context. Always include specific references to \"\n",
    "                    \"source documents and sections when possible. Format references \"\n",
    "                    \"as {filename, section}: content. If the context doesn't contain \"\n",
    "                    \"relevant information, say so clearly.\"\n",
    "                )\n",
    "            )\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama setup failed: {e}\")\n",
    "            print(\"Falling back to HuggingFace implementation...\")\n",
    "            \n",
    "        # Option 2: Using HuggingFace Transformers directly\n",
    "        return self._setup_hf_llm()\n",
    "    \n",
    "    def _setup_hf_llm(self):\n",
    "        \"\"\"\n",
    "        Setup HuggingFace LLM for local inference.\n",
    "        \n",
    "        Returns:\n",
    "            A HuggingFaceLLM instance configured for local inference.\n",
    "            \n",
    "        Note:\n",
    "            Uses 8-bit quantization for memory efficiency.\n",
    "            Requires sufficient GPU memory for inference.\n",
    "        \"\"\"\n",
    "        model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        \n",
    "        # System prompt for RAG with source attribution\n",
    "        system_message = (\n",
    "            \"You are a helpful assistant. Use the provided context to answer \"\n",
    "            \"questions accurately. Always include specific references to source \"\n",
    "            \"documents when possible using format {filename, section}: content. \"\n",
    "            \"If the context doesn't contain relevant information, say so clearly.\"\n",
    "        )\n",
    "        \n",
    "        # Query wrapper for Mistral format\n",
    "        query_wrapper_prompt = (\n",
    "            f\"<s>[INST] {system_message}\\n\\n\"\n",
    "            \"Context: {context_str}\\n\\n\"\n",
    "            \"Question: {query_str} [/INST]\"\n",
    "        )\n",
    "        \n",
    "        llm = HuggingFaceLLM(\n",
    "            model_name=model_name,\n",
    "            tokenizer_name=model_name,\n",
    "            query_wrapper_prompt=query_wrapper_prompt,\n",
    "            context_window=4096,\n",
    "            max_new_tokens=DEFAULT_MAX_TOKENS,\n",
    "            model_kwargs={\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"load_in_8bit\": True,  # For memory efficiency\n",
    "            },\n",
    "            tokenizer_kwargs={},\n",
    "            generate_kwargs={\n",
    "                \"temperature\": DEFAULT_TEMPERATURE,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.9,\n",
    "            },\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        return llm\n",
    "    \n",
    "    def _setup_embeddings(self):\n",
    "        \"\"\"\n",
    "        Setup local embedding model.\n",
    "        \n",
    "        Returns:\n",
    "            A HuggingFaceEmbedding instance for generating text embeddings.\n",
    "        \"\"\"\n",
    "        return HuggingFaceEmbedding(\n",
    "            model_name=self.embed_model_name,\n",
    "            cache_folder=\"./embeddings_cache\"\n",
    "        )\n",
    "    \n",
    "    def load_pdf_documents(self, pdf_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load multiple PDF documents with filename metadata.\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths: List of paths to PDF files to load.\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects with metadata.\n",
    "            \n",
    "        Note:\n",
    "            Each document includes metadata about its source file and page number.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"Warning: PDF file {pdf_path} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Load PDF using PDFReader\n",
    "                pdf_docs = self.pdf_reader.load_data(Path(pdf_path))\n",
    "                \n",
    "                # Add filename metadata to each document\n",
    "                filename = os.path.basename(pdf_path)\n",
    "                for doc in pdf_docs:\n",
    "                    doc.metadata[\"filename\"] = filename\n",
    "                    doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                    # Add page number if available\n",
    "                    if \"page_label\" in doc.metadata:\n",
    "                        doc.metadata[\"reference\"] = (\n",
    "                            f\"{filename}, page {doc.metadata['page_label']}\"\n",
    "                        )\n",
    "                    else:\n",
    "                        doc.metadata[\"reference\"] = filename\n",
    "                \n",
    "                documents.extend(pdf_docs)\n",
    "                print(f\"Loaded PDF: {filename} ({len(pdf_docs)} pages)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading PDF {pdf_path}: {e}\")\n",
    "                \n",
    "        return documents\n",
    "    \n",
    "    def load_docx_documents(self, docx_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load multiple Word documents with filename metadata.\n",
    "        \n",
    "        Args:\n",
    "            docx_paths: List of paths to DOCX files to load.\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects with metadata.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for docx_path in docx_paths:\n",
    "            if not os.path.exists(docx_path):\n",
    "                print(f\"Warning: DOCX file {docx_path} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Load DOCX using DocxReader\n",
    "                docx_docs = self.docx_reader.load_data(Path(docx_path))\n",
    "                \n",
    "                # Add filename metadata\n",
    "                filename = os.path.basename(docx_path)\n",
    "                for doc in docx_docs:\n",
    "                    doc.metadata[\"filename\"] = filename\n",
    "                    doc.metadata[\"source_type\"] = \"docx\"\n",
    "                    doc.metadata[\"reference\"] = filename\n",
    "                \n",
    "                documents.extend(docx_docs)\n",
    "                print(f\"Loaded DOCX: {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading DOCX {docx_path}: {e}\")\n",
    "                \n",
    "        return documents\n",
    "    \n",
    "    def load_mixed_documents(self, data_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load documents from directory - supports PDF, DOCX, TXT, MD.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to directory containing documents.\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects with enhanced metadata.\n",
    "            \n",
    "        Note:\n",
    "            Uses SimpleDirectoryReader to automatically handle multiple file types.\n",
    "            Adds enhanced metadata for better source attribution.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path {data_path} not found\")\n",
    "        \n",
    "        # SimpleDirectoryReader supports PDF, DOCX, TXT, MD, HTML, and more\n",
    "        reader = SimpleDirectoryReader(\n",
    "            input_dir=data_path,\n",
    "            recursive=True,  # Search subdirectories\n",
    "            required_exts=[\".pdf\", \".docx\", \".txt\", \".md\"],  # Supported file types\n",
    "        )\n",
    "        \n",
    "        documents = reader.load_data()\n",
    "        \n",
    "        # Add enhanced metadata for better source attribution\n",
    "        for doc in documents:\n",
    "            if \"file_path\" in doc.metadata:\n",
    "                filepath = doc.metadata[\"file_path\"]\n",
    "                filename = os.path.basename(filepath)\n",
    "                doc.metadata[\"filename\"] = filename\n",
    "                \n",
    "                # Determine file type\n",
    "                if filename.lower().endswith('.pdf'):\n",
    "                    doc.metadata[\"source_type\"] = \"pdf\"\n",
    "                    # Add page info if available\n",
    "                    if \"page_label\" in doc.metadata:\n",
    "                        doc.metadata[\"reference\"] = (\n",
    "                            f\"{filename}, page {doc.metadata['page_label']}\"\n",
    "                        )\n",
    "                    else:\n",
    "                        doc.metadata[\"reference\"] = filename\n",
    "                elif filename.lower().endswith('.docx'):\n",
    "                    doc.metadata[\"source_type\"] = \"docx\"\n",
    "                    doc.metadata[\"reference\"] = filename\n",
    "                else:\n",
    "                    doc.metadata[\"source_type\"] = \"text\"\n",
    "                    doc.metadata[\"reference\"] = filename\n",
    "        \n",
    "        print(f\"Loaded {len(documents)} document chunks from {data_path}\")\n",
    "        return documents\n",
    "    \n",
    "    def create_index(self, documents: List[Document], persist_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Create vector index from documents with enhanced metadata.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects to index.\n",
    "            persist_dir: Optional directory to save the index.\n",
    "            \n",
    "        Returns:\n",
    "            A VectorStoreIndex instance.\n",
    "            \n",
    "        Note:\n",
    "            The index includes enhanced metadata for better source attribution\n",
    "            and chunk relationships.\n",
    "        \"\"\"\n",
    "        # Parse documents into nodes with metadata preservation\n",
    "        node_parser = SimpleNodeParser.from_defaults(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            include_metadata=True,  # Preserve document metadata\n",
    "            include_prev_next_rel=True  # Track chunk relationships\n",
    "        )\n",
    "        \n",
    "        nodes = node_parser.get_nodes_from_documents(documents)\n",
    "        \n",
    "        # Enhance node metadata for better source attribution\n",
    "        for node in nodes:\n",
    "            if hasattr(node, 'metadata') and node.metadata:\n",
    "                # Create a readable reference string\n",
    "                if \"reference\" in node.metadata:\n",
    "                    reference = node.metadata[\"reference\"]\n",
    "                else:\n",
    "                    reference = node.metadata.get(\"filename\", \"Unknown source\")\n",
    "                \n",
    "                # Add chunk identifier\n",
    "                if hasattr(node, 'node_id'):\n",
    "                    node.metadata[\"chunk_id\"] = node.node_id[:8]  # Short ID\n",
    "                    \n",
    "                node.metadata[\"full_reference\"] = reference\n",
    "        \n",
    "        # Create index\n",
    "        index = VectorStoreIndex(\n",
    "            nodes,\n",
    "            service_context=self.service_context,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        # Persist index if path provided\n",
    "        if persist_dir:\n",
    "            os.makedirs(persist_dir, exist_ok=True)\n",
    "            index.storage_context.persist(persist_dir)\n",
    "            print(f\"Index saved to {persist_dir}\")\n",
    "            \n",
    "        return index\n",
    "    \n",
    "    def load_index(self, persist_dir: str):\n",
    "        \"\"\"\n",
    "        Load existing index from storage.\n",
    "        \n",
    "        Args:\n",
    "            persist_dir: Directory containing the saved index.\n",
    "            \n",
    "        Returns:\n",
    "            A VectorStoreIndex instance loaded from storage.\n",
    "        \"\"\"\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index = load_index_from_storage(\n",
    "            storage_context,\n",
    "            service_context=self.service_context\n",
    "        )\n",
    "        print(f\"Index loaded from {persist_dir}\")\n",
    "        return index\n",
    "    \n",
    "    def query(self, index, question: str, include_sources: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the RAG system with detailed source attribution.\n",
    "        \n",
    "        Args:\n",
    "            index: VectorStoreIndex instance to query.\n",
    "            question: Question to ask.\n",
    "            include_sources: Whether to include source information in response.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - answer: The generated answer\n",
    "            - sources: List of source chunks used (if include_sources=True)\n",
    "            \n",
    "        Note:\n",
    "            Each source includes:\n",
    "            - rank: Position in retrieval results\n",
    "            - score: Similarity score\n",
    "            - reference: Source document reference\n",
    "            - content_preview: Preview of source content\n",
    "            - metadata: Full source metadata\n",
    "        \"\"\"\n",
    "        query_engine = index.as_query_engine(\n",
    "            similarity_top_k=self.top_k,\n",
    "            service_context=self.service_context,\n",
    "            response_mode=\"compact\"  # Better for source attribution\n",
    "        )\n",
    "        \n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        result = {\n",
    "            \"answer\": str(response),\n",
    "            \"sources\": []\n",
    "        }\n",
    "        \n",
    "        # Extract source information\n",
    "        if include_sources and hasattr(response, 'source_nodes'):\n",
    "            for i, node in enumerate(response.source_nodes):\n",
    "                source_info = {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"score\": getattr(node, 'score', 0.0),\n",
    "                    \"reference\": node.metadata.get(\"full_reference\", \"Unknown\"),\n",
    "                    \"content_preview\": (\n",
    "                        node.text[:200] + \"...\" if len(node.text) > 200 else node.text\n",
    "                    ),\n",
    "                    \"metadata\": dict(node.metadata)\n",
    "                }\n",
    "                result[\"sources\"].append(source_info)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run doctests\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
