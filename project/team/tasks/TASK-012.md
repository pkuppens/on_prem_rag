# Task: Integrate Ollama LLM for Answer Generation

**ID**: TASK-012
**Story**: [STORY-003: Basic Q&A Interface](../stories/STORY-003.md)
**Assignee**: ML Engineer
**Status**: Todo
**Effort**: 6 hours
**Created**: 2025-05-31
**Updated**: 2025-05-31

## Description
Use the Ollama local LLM runtime to generate answers based on retrieved document context. Provide a simple prompt template for initial testing.

## Acceptance Criteria
- [ ] Function accepts question and context and returns generated answer text
- [ ] Errors from the LLM service are gracefully handled

## Dependencies
- **Blocked by**: TASK-010 (API endpoint scaffolding)

---
**Implementer**: ML Engineer
**Reviewer**: Lead Developer
**Target Completion**: TBD
