{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Document Loading and Chunking Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Loading PDF documents using the RAG pipeline\n",
    "- Page-based chunking with token size considerations\n",
    "- Detailed reporting of chunking results\n",
    "- Using FAST_ANSWERS configuration parameters\n",
    "\n",
    "We'll use a test PDF document to show the complete process from loading to chunking analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_notebook  # This fixes the path for imports\n",
    "from rag_pipeline.config.parameter_sets import FAST_ANSWERS\n",
    "from rag_pipeline.core.embeddings import process_pdf\n",
    "from rag_pipeline.utils.directory_utils import get_project_root, get_test_data_dir\n",
    "\n",
    "# Use FAST_ANSWERS configuration for this example\n",
    "rag_params = FAST_ANSWERS\n",
    "\n",
    "# Set up paths\n",
    "root = get_project_root()\n",
    "test_data = get_test_data_dir()\n",
    "pdf_path = test_data / \"2303.18223v16.pdf\"\n",
    "persist_dir = root / \"data\" / \"test_chunks\"\n",
    "\n",
    "# Ensure persist directory exists\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Processing the PDF Document\n",
    "\n",
    "We'll now process the PDF document using the following steps:\n",
    "1. Load the PDF using our custom processor\n",
    "2. Apply page-based chunking with rag_params = FAST_ANSWERS parameters:\n",
    "   - Chunk size: rag_params.chunking.chunk_size tokens\n",
    "   - Chunk overlap: rag_params.chunking.chunk_overlap tokens\n",
    "   - Chunk size should not exceed a single PDF page for PDF files, multiple chunks should then be created for a single page\n",
    "3. Enable deduplication to avoid redundant content processing\n",
    "   - Deduplication here means that we don't want to process the same PDF file with the same chunking parameters (We don't expect duplicate chunks in the same PDF)\n",
    "   - Chunking the same file with the same parameters should be idempotent\n",
    "   - Suggestions:\n",
    "      - keep the chunking database should be different from the embedding database\n",
    "      - use the chunking parameters in the database name\n",
    "      - check the database for any records with the given file name to detect if chunking was already executed\n",
    "      - apply embeddings on the chunking database records when needed\n",
    "4. Process all pages to get a complete view of the document structure\n",
    "\n",
    "The process_pdf function will return:\n",
    "- File name, file size, file number of pages\n",
    "- Total number of chunks created\n",
    "- Database: file name, table name, and other relevant attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the PDF with full page processing and deduplication\n",
    "if not (persist_dir / \"chroma.sqlite3\").exists():\n",
    "    chunks, records = await process_pdf(\n",
    "        pdf_path,\n",
    "        rag_params.embedding.model_name,\n",
    "        persist_dir=str(persist_dir),\n",
    "        chunk_size=rag_params.chunking.chunk_size,\n",
    "        chunk_overlap=rag_params.chunking.chunk_overlap,\n",
    "        max_pages=None,  # Process all pages\n",
    "        deduplicate=True,\n",
    "    )\n",
    "    print(f\"PDF Processing Results:\")\n",
    "    print(f\"----------------------\")\n",
    "    print(f\"Total chunks created: {chunks}\")\n",
    "    print(f\"Unique records after deduplication: {records}\")\n",
    "    print(f\"Deduplication removed: {chunks - records} chunks\")\n",
    "    print(f\"Deduplication ratio: {(chunks - records) / chunks:.2%}\")\n",
    "else:\n",
    "    print(f\"Embeddings already exist in {persist_dir}, skipping processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_pipeline.core.chunking import get_page_chunks\n",
    "\n",
    "# Get chunks per page analysis\n",
    "page_chunks = get_page_chunks(pdf_path)\n",
    "\n",
    "print(f\"Page-Level Analysis:\")\n",
    "print(f\"-----------------\")\n",
    "print(f\"Total pages in document: {len(page_chunks)}\")\n",
    "print(f\"Average chunks per page: {sum(len(chunks) for chunks in page_chunks.values()) / len(page_chunks):.2f}\")\n",
    "print(f\"\\nPages with most chunks:\")\n",
    "sorted_pages = sorted(page_chunks.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "for page_num, chunks in sorted_pages[:5]:\n",
    "    print(f\"Page {page_num}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nPages with least chunks:\")\n",
    "for page_num, chunks in sorted_pages[-5:]:\n",
    "    print(f\"Page {page_num}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Analysis Summary\n",
    "\n",
    "The above results show:\n",
    "1. The complete document processing with page-by-page chunking\n",
    "2. Deduplication effectiveness in removing redundant content\n",
    "3. Distribution of chunks across pages, highlighting:\n",
    "   - Pages with dense content (more chunks)\n",
    "   - Pages with sparse content (fewer chunks)\n",
    "   - Average chunk distribution\n",
    "\n",
    "This analysis helps understand how the document is being processed and can be used to tune the chunking parameters if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
